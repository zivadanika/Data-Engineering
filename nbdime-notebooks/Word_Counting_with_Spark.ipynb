{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKJWSvSKOl1c"
   },
   "source": [
    "# PySpark Demo and Word Counting with Spark\n",
    "\n",
    "To get you started, we'll walk you through a bit of Colab specific Python and some PySpark code, and then we'll do the classic word count example, followd by some tasks for you to try.\n",
    "\n",
    "**Please run through the notebook cell by cell (using 'run' above or 'shift-return' on the keyboard).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "95B3FYvqPya6"
   },
   "source": [
    "##Preliminaries: Preparing Colab and Spark\n",
    "1.   When you open this notebook from the shared \"Data-Engineering\" folder, you don't have write acceess. When you save it, a copy will be created in the folder \"Colab Notebooks\".\n",
    "2.   The code below will mount Google Drive as a directory in the file system (the machine is a virtual Linux box). You will be asked to authorise this and provide an authentication code available through a link.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "8edGFcfkPx50",
    "outputId": "1b8d8ccd-8d6c-4876-8372-bd21c1ea5f9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GpO-PaoGUID4"
   },
   "source": [
    "Next we move to the \"Colab Notebooks\" folder on your drive and create subfolder \"data\". Then we copy the \"hamlet.txt\" file there (you can check on Google Drive if it has worked). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "_Bn28SFjUHC_",
    "outputId": "2530e5ff-9f87-4fc7-d226-7692103d7df7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\n",
      "drwx------ 4 root root 4096 Jan 28 18:43 drive\n",
      "drwxr-xr-x 1 root root 4096 Jan 13 16:38 sample_data\n"
     ]
    }
   ],
   "source": [
    "# %cd /content/drive/My\\ Drive/\n",
    "# !mkdir \"Colab Notebooks\"\n",
    "# %cd \"Colab Notebooks\"\n",
    "# !mkdir data\n",
    "# %cd data\n",
    "# !cp \"/content/drive/My Drive/Data-Engineering/data/hamlet.txt\" .\n",
    "\n",
    "# instead of copying in the script above, upload the hamlet.txt file into the path above using drive\n",
    "# you will need to create the above folder structure\n",
    "\n",
    "# if you use a different naming scheme you will need to adjust the fileName as necessary\n",
    "# use the 'Files' tab of the menu that is hidden on the left of the screen to see the file system structure\n",
    "\n",
    "# find out the working directory\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fpUd_fHJRInX"
   },
   "source": [
    "Next, we Install Spark (may take a while) and altair for visualisaion. This will need to be done every time a new machine is created. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "2S9ShIHjSlDS",
    "outputId": "3571d07f-0e31-4d9f-b067-e342ecf42837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
      "\u001b[K     |████████████████████████████████| 215.7MB 64kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 49.7MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130387 sha256=b5a1a0c91bfca53fb03615f766c8f4d0cf7533223f99ce3fdd4b9c42a80f8dd3\n",
      "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.4\n",
      "Requirement already satisfied: altair in /usr/local/lib/python3.6/dist-packages (4.0.0)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from altair) (2.6.0)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.6/dist-packages (from altair) (0.10.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from altair) (1.17.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from altair) (2.10.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from altair) (0.25.3)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from altair) (0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->altair) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->altair) (2.6.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->altair) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->altair) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "# Install spark-related dependencies\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# !wget -q http://www-eu.apache.org/dist/spark/spark-2.3.3/spark-2.3.3-bin-hadoop2.7.tgz\n",
    "!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
    "\n",
    "!pip install -q findspark\n",
    "!pip install pyspark\n",
    "# Set up required environment variables\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-9-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
    "#!pip install pyspark\n",
    "!pip install altair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lVuFYFEmjWjx"
   },
   "source": [
    "If the installation above doesnn't work, try the one below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rmJvtPxjOl1e"
   },
   "source": [
    "## Part 1 - Demo: Apapche Spark API with PySpark\n",
    "\n",
    "Basically there are 32APIs available in Apache Spark - RDD (Resilient Distributed Datasets) and DataFrame (extended by Dataset in Scala and Java). In this lab we will look at RDDs and Dataframes in Python.\n",
    "\n",
    "For more information on the Spark framework - visit (https://spark.apache.org)\n",
    "For more information on the Pyspark API - visit (https://spark.apache.org/docs/latest/api/python/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IdIBCxgFgFQT",
    "outputId": "a057f65c-0ca2-4090-b665-d60f0a6ab7f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zmoNsdqQOl1f"
   },
   "source": [
    "### 1) Access to Spark\n",
    "\n",
    "We start by cretaing a SparkContext, normally called `sc`. \n",
    "We use that to create RDDs and a SparkSession object (for DataFrames), often just called `spark`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "12aW4ncAOl1h",
    "outputId": "9781477b-6d54-41d0-93c6-2dcaad63181f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n",
      "<pyspark.sql.session.SparkSession object at 0x7fcba6d5e518>\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "# get a spark context\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "print(sc)\n",
    "# get the context\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "print(spark) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ctb3qeAkOl1m"
   },
   "source": [
    "### 2) RDD Creation\n",
    "\n",
    "There are two ways to create RDDs. The first is to parallelise a Python object that exists in your driver process (i.e. this one). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jbuuRZv6Ol1n"
   },
   "source": [
    "The second way is to create an RDD is by referencing an external dataset such as a shared filesystem, HDFS, HBase, or just data source offering a Hadoop InputFormat. This is what we will be using in this lab (further down)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "phwtMBs2Ol1p",
    "outputId": "d35399ab-ea8f-4960-b638-815c571bbd15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195\n"
     ]
    }
   ],
   "source": [
    "# Creat an RDD from a Python object in this process (the \"driver\").\n",
    "# The parallelize function  creating the \"numbers\" RDD\n",
    "data = [1,2,3,4,5]\n",
    "firstRDD = sc.parallelize(data)\n",
    "print(firstRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sx75RI9mOl1s"
   },
   "source": [
    "This RDD lives now on as many worker machines as are available and as are deemed useful by Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cDnZCdSTOl1t"
   },
   "source": [
    "### 3) RDD operations\n",
    "RDDs have two kinds of operations: *Transformations* and *Actions*.\n",
    "\n",
    "*Transformations* create a new RDD by applying a function to the items in the RDD. The function will be remembered, but only be applied when needed (\"*lazy* evaluation\").\n",
    "\n",
    "*Actions* produce some output from the data. An *Action* will trigger the execution of all *Transformations*.\n",
    "\n",
    "Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Q25kDkt9Ol1v",
    "outputId": "6931010a-57d5-4227-d775-def2034dbfb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[1] at RDD at PythonRDD.scala:53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myfun(x):\n",
    "  return x+3\n",
    "# lambda function: x -> x+3\n",
    "#RDD2 = firstRDD.map(lambda x:x+3)  \n",
    "RDD2 = firstRDD.map(myfun)  \n",
    "print(RDD2)\n",
    "# nothing happened to far, as there is no action\n",
    "#RDD2.count()\n",
    "RDD2.countApprox(10000, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QhIIytM9bSvm"
   },
   "source": [
    "If the functions are short (one expression, to be exact), this is more convenient write with a lamba expression, that creates an anonymous function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IEtzNnm4bZfe",
    "outputId": "7d357fd0-7fa8-4675-bbdf-8e39b02b60a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[5] at RDD at PythonRDD.scala:53\n"
     ]
    }
   ],
   "source": [
    "RDD3 = firstRDD.map(lambda x:x+3) # this is the same as using myfun \n",
    "print(RDD3)\n",
    "# nothing happened to far, as there is no action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ju4PvD5gOl10",
    "outputId": "5386dac2-79fa-4234-a435-65eebbd3ad7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# \"count\" is an action and triggers the transformation   \n",
    "a = RDD2.count() \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6xp2aTh8Ol15"
   },
   "source": [
    "`collect` is an action that returns the values of the RDD in an Python array, back into this local driver process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "tdaNRc0SOl15",
    "outputId": "631cef9a-1746-4dec-a642-580011e4e10f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 6, 7, 8]\n",
      "[4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "a = RDD2.collect() \n",
    "print(a)\n",
    "\n",
    "b = RDD3.collect() \n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXmtXG0hOl19"
   },
   "source": [
    "As we can seee above, *myfun* (RDD2) and the *lambda x: x+3* (RDD3) have the same effect.\n",
    "\n",
    "Look here for more information about the functions provided by the RDD class: (https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.RDD). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUl3upk0Ol1-"
   },
   "source": [
    "### 4) Dataframes \n",
    "\n",
    "Dataframes are a more structured form of storage than RDDs and similar to Pandas dataframes.  \n",
    "\n",
    "Let us see how to create and use dataframes. There are three ways of creating a dataframe\n",
    "    a) from an existing RDD.\n",
    "    b) form and external data source, e.g., loading the data from JSON or CSV files.\n",
    "    c) Programmatically specifying schema and data.\n",
    "    \n",
    "Here is an example for option a). We use the *Row* class to create structure data rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hSIzFqAQOl1_",
    "outputId": "034c768c-4967-4f2b-a4a7-49270071170c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[age: bigint, name: string]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "dataList = [('Anne',21),('Bob',22),('Carl',29),('Daisy',36)] # our data as a list\n",
    "rdd = sc.parallelize(dataList) # RDD from the list\n",
    "peopleRDD = rdd.map(lambda x: Row(name=x[0], age=int(x[1]))) # RDD\n",
    "peopleDF = spark.createDataFrame(peopleRDD) \n",
    "print(peopleDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsGAOxX2Ol2G"
   },
   "source": [
    "## Part 2: Classic Word Count\n",
    "\n",
    "We will now do the classic word count example for the MapReduce pattern.\n",
    "\n",
    "We will apply it to the text of Sheakespeare's play *Hamlet*. For that you should have uploaded the file \"hamlet.txt\" into the data assets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-eWjGlkbOl2H"
   },
   "source": [
    "### 6) Load the data\n",
    "First we need to load the text into an RDD (the second method of creating an RDD as mentioned above). \n",
    "\n",
    "We need to specify the path, and we can read directly from the shared Data-Engineering directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kzdsuhxtOl2H"
   },
   "outputs": [],
   "source": [
    "# filepath = \"/content/drive/My Drive/Data-Engineering/data/hamlet.txt\"\n",
    "# use a relative path to the file\n",
    "filepath = \"drive/My Drive/Data-Engineering/data/hamlet.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "1uG51YrzlH7a",
    "outputId": "38156f40-05d1-49bd-bc6e-7f6684df2d16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 449594\n",
      "drwx------ 2 root root      4096 Jan 22 11:41 data\n",
      "-r-------- 1 root root    193083 Jan 22 10:19 hamlet.txt\n",
      "dr-x------ 2 root root      4096 Jan 22 11:10 spark-2.4.4-bin-hadoop2.7\n",
      "-r-------- 1 root root 230091034 Aug 27 22:01 spark-2.4.4-bin-hadoop2.7.tgz\n",
      "-r-------- 1 root root 230091034 Aug 27 22:01 spark-2.4.4-bin-hadoop2.7.tgz.1\n"
     ]
    }
   ],
   "source": [
    "# verify that the file is there\n",
    "!ls -l \"drive/My Drive/Data-Engineering/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5q8UiCtOl2J"
   },
   "source": [
    "You can read the file into an RDD with `textFile`. The RDD then contains as items the lines of the text. `take(3)` then gives us the first 3 lines.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "OM_vQDsGOl2J",
    "outputId": "4bd6466b-bc80-4275-e2e3-e8562c518e87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project Gutenberg Etext of Hamlet by Shakespeare',\n",
       " \"PG has multiple editions of William Shakespeare's Complete Works\",\n",
       " '']"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineRDD = sc.textFile(filepath)\n",
    "lineRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ab6vKwgvOl2L"
   },
   "source": [
    "### 7) Split lines into words\n",
    "\n",
    "In order to count the words, we need to split the lines into words. We can do that using the `split` function of the Python String class to separate at each space. \n",
    "\n",
    "The map function replaces each item with a new one, in this case our `lambda` returns an array of words (provided by `split(' ')`). However, we want to create one item per word, therefore we need to use a function called `flatMap` that creates a new RDD item for every item in the array returned by the lambda.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WwI2cDlSOl2L",
    "outputId": "6a1300c4-56b6-4a75-99ca-db4dad4b4f25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project', 'Gutenberg', 'Etext']"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordRDD = lineRDD.flatMap(lambda x: x.split(' '))\n",
    "wordRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jDuHh_ocOl2N"
   },
   "source": [
    "Map the words to tuples of the form *(word, 1)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0PXZXAGpOl2O",
    "outputId": "4af8a9c7-ba4f-4e7f-c5cf-ed54b5503ef4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Project', 1), ('Gutenberg', 1), ('Etext', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1RDD = wordRDD.map(lambda x: (x, 1))\n",
    "word1RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5BplN07GOl2P"
   },
   "source": [
    "### 8) Count by reducing\n",
    "For Spark, the first part in each tuple is the 'key'. Now we can use reduceByKey() to add the 1s and get the number of occurences per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HRFaQARsOl2Q",
    "outputId": "c3cac69e-da23-45ac-e922-22cf74926197"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Project', 21), ('Etext', 4), ('of', 679)]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCountRDD = word1RDD.reduceByKey(lambda x,y: x+y )\n",
    "wordCountRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G38rUrlVOl2S"
   },
   "source": [
    "### 9) Filtering \n",
    "\n",
    "There are many empty strings returned by the splitting. We can remove them by filtering.\n",
    "Then can take a shortcut and use a ready-made functions 'count by value', which does the same as we before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZQ9mBefpOl2S",
    "outputId": "b31ed6fb-409f-4717-d809-7f7cdc8e5721"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Project', 21), ('Etext', 4), ('of', 679), ('Shakespeare', 5), ('multiple', 3)]\n"
     ]
    }
   ],
   "source": [
    "wordFilteredRDD = wordRDD.filter(lambda x: len(x)>0)\n",
    "word1RDD = wordFilteredRDD.map(lambda x: (x, 1))\n",
    "wordCountRDD = word1RDD.reduceByKey(lambda x,y: x+y )\n",
    "wcList = wordCountRDD.take(5)\n",
    "print(wcList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oyubLNalOl2W"
   },
   "source": [
    "## Part 3: Tasks for you to work on\n",
    "\n",
    "Based on the examples above, you can now try and write some code yourself.  Look for the lines starting with **>>>**. You neeed to fix them by writing your own code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LY4f9UwOl2W"
   },
   "source": [
    "## Task 1) Better splitting \n",
    "\n",
    "Currently our 'words' can contain punctuation, becausee only spaces are removed. A better way to split is using regular expressions  (Python's 're' package)(https://docs.python.org/3.5/library/re.html?highlight=regular%20expressions). `re.split('\\W+', 'my. test. string!')` does a good job. Try it out below by fixing the line that starts with '>>>'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wuQDbhOAOl2X",
    "outputId": "f494d2dd-f2ce-4d5f-c365-482bc0369c44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project', 'Gutenberg', 'Etext']"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "#>>> wordRDD = lineRDD.flatMap(lambda x: ...)\n",
    "wordRDD = lineRDD.flatMap(lambda x: re.split('\\W+', x)) # apply re.split('\\W+', string) here\n",
    "wordFilteredRDD = wordRDD.filter(lambda x: len(x)>0) # filtering\n",
    "wordFilteredRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ktBnkI5TOl2Y"
   },
   "source": [
    "## 2) Use lower case\n",
    "\n",
    "Convert all strings to lower case (using `.lower()` provided by the Python string class), so that 'Test' and 'test' count as the same. Package it into one a tuple of the form (word,1) in the same call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DBqa2EQjOl2Y",
    "outputId": "18feeace-fdf4-4fd6-f659-bc03e2fd7255"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project', 'gutenberg', 'etext']"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#>>> wordLowerRDD = wordFilteredRDD.map(lambda x: ... )\n",
    "wordLowerRDD = wordFilteredRDD.map(lambda x: x.lower() )\n",
    "wordLowerRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "WIVwV56lOl2a",
    "outputId": "99684437-0a80-4951-955c-713701540e63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 36),\n",
       " ('gutenberg', 27),\n",
       " ('of', 733),\n",
       " ('shakespeare', 7),\n",
       " ('multiple', 3)]"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1RDD = wordLowerRDD.map(lambda x: (x,1)) # we can now get better word count results\n",
    "wordCountRDD = word1RDD.reduceByKey(lambda x,y: x+y) # we can now get better word count results\n",
    "wordCountRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7jgnYSVOl2b"
   },
   "source": [
    "## 3) Filter rare words\n",
    "\n",
    "Add a filtering step call remove all words with less than 5 occurrences. This can be useful to identify common topics in documents, where very rare words can be misleading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "CTd9S_y6Ol2b",
    "outputId": "381692d1-23c3-4ac2-aa8f-c3ae7c018724"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 36),\n",
       " ('gutenberg', 27),\n",
       " ('of', 733),\n",
       " ('shakespeare', 7),\n",
       " ('s', 251)]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the trick here is to apply the lambda only to the second part of each item, i.e. x[1] \n",
    "#freqWordsRDD = wordCountRDD.filter(lambda x:  ... ) # tip: filter keeps the times where the lambda returns true.\n",
    "freqWordsRDD = wordCountRDD.filter(lambda x:  x[1]>5 ) # tip: filter keeps the times where the lambda returns true.\n",
    "freqWordsRDD.take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IypcO9SPOl2d"
   },
   "source": [
    "## 4) List only stopwords\n",
    "\n",
    "Stopwords are frequent words that are not topic-specifc.  Stopwords can be useful for recognising the style of an author. Removing stopwords can be useful in regocnising the topic of a document. \n",
    "\n",
    "Below is a small list of stopwords. Filter the tuples where the first part is a stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "T85Nx6ZuOl2e",
    "outputId": "a8c3e075-ef20-4aa6-b621-ed8cf74512a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 733),\n",
       " ('at', 99),\n",
       " ('in', 464),\n",
       " ('by', 147),\n",
       " ('the', 1218),\n",
       " ('for', 277),\n",
       " ('a', 582),\n",
       " ('you', 610),\n",
       " ('on', 150),\n",
       " ('me', 236)]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWordList = ['the','a','in','of','on','at','for','by','I','you','me'] \n",
    "#stopWordsRDD = freqWordsRDD.filter(lambda x:  ...) # the 1st part of the tuple should be in the list\n",
    "stopWordsRDD = freqWordsRDD.filter(lambda x: x[0] in stopWordList ) # the 1st part of the tuple should be in the list \n",
    "\n",
    "stopWordsRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-rysKdLffGTn",
    "outputId": "380228a1-5380-42e5-8a5a-83f21e03bc73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWordsRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gKVS18B9Ol2f"
   },
   "source": [
    "There are only a few words, so we can see the vies results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "JsX57MXHOl2g",
    "outputId": "3f6f4057-6a53-432c-d9d8-ff4a7b8f1190"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of: 733\n",
      "at: 99\n",
      "in: 464\n",
      "by: 147\n",
      "the: 1218\n",
      "for: 277\n",
      "a: 582\n",
      "you: 610\n",
      "on: 150\n",
      "me: 236\n"
     ]
    }
   ],
   "source": [
    "output = stopWordsRDD.collect() \n",
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJcV-ulGOl2i"
   },
   "source": [
    "We can now visualise the stopword counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "Y3p___CROl2i",
    "outputId": "5fe5a2d9-a4c4-46b6-fbc1-6c92936352ac",
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "handlerId": "barChart",
      "keyFields": "_1",
      "rowCount": "500",
      "valueFields": "_2"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>items</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>of</td>\n",
       "      <td>733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>at</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>by</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the</td>\n",
       "      <td>1218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>for</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a</td>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>you</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>on</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>me</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  words  items\n",
       "0    of    733\n",
       "1    at     99\n",
       "2    in    464\n",
       "3    by    147\n",
       "4   the   1218\n",
       "5   for    277\n",
       "6     a    582\n",
       "7   you    610\n",
       "8    on    150\n",
       "9    me    236"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "word_list=[]\n",
    "item_list=[]\n",
    "for item in output:\n",
    "  (word,count)=item\n",
    "  word_list.append(word)\n",
    "  item_list.append(count)\n",
    "df3=pd.DataFrame({'words':word_list,'items':item_list})\n",
    "df3"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Revised Word_Counting_with_Spark - Solutions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
